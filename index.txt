我们来挑战一个更高级的任务：从《水浒传》电视剧中提取宋江的声音。

你提到的问题非常关键：“视频里会有多个人的声音，有背景音乐等等”。这正是这个任务的核心难点，也让它从一个简单的文本处理问题，升级为了一个复杂的音频处理和机器学习问题。

别担心，我会为你分解这个过程，并提供清晰的步骤和代码指导。

整体方案概述
与上次从文本提取不同，这次我们需要“听”和“分辨”。整个流程可以分解为以下几个关键阶段：

准备素材：从视频文件中提取出原始的音频轨道。

声源分离 (Source Separation)：这是一个可选但强烈推荐的步骤。我们尝试将音频中的人声与背景音乐、环境音等分离开。

说话人日志 (Speaker Diarization)：这是最核心的一步。我们需要分析纯人声音频，搞清楚“在哪个时间段，是哪个人在说话”。计算机会自动给说话人贴上标签，比如“说话人A”、“说话人B”。

说话人识别 (Speaker Identification)：我们有了不同说话人的音频片段，但不知道哪个是宋江。所以，我们需要提供一小段“宋江”的纯净声音样本（我们称之为“声纹”），让程序去匹配，找出所有属于宋江的片段。

合并导出：将所有识别为宋江的音频片段合并起来，导出最终文件。

对于这个任务，Python 是绝对的主力，因为它有非常强大的音频处理和机器学习库。N8N、Dify 这类流程自动化工具在这里就不太适用了，因为它们无法进行如此底层的、计算密集型的媒体文件分析。

第一步：准备工作 - 提取音频和安装工具
1. 准备视频文件
首先，你需要有《水浒传》电视剧的视频文件（比如 .mp4 格式）。假设你有一个名为 episode_01.mp4 的文件。

2. 安装 FFmpeg
FFmpeg 是一个处理音视频的瑞士军刀，我们可以用它轻松地从视频中提取音频。

Windows: 下载 FFmpeg 并将其 bin 目录添加到系统环境变量 PATH 中。

macOS: 使用 Homebrew 安装：brew install ffmpeg

Linux: 使用包管理器安装：sudo apt update && sudo apt install ffmpeg

安装后，打开终端，运行以下命令将视频中的音频提取为 .wav 文件（WAV是无损格式，更适合后续分析）：

Bash

ffmpeg -i episode_01.mp4 -vn -acodec pcm_s16le -ar 16000 -ac 1 episode_01_audio.wav
命令解释:

-i episode_01.mp4: 指定输入文件。

-vn: 表示丢弃视频轨道。

-acodec pcm_s16le: 设置音频编码为16位的WAV格式。

-ar 16000: 设置采样率为 16000 Hz。很多语音模型都使用这个采样率。

-ac 1: 设置为单声道。

episode_01_audio.wav: 指定输出文件名。

完成这一步，你就得到了一个纯音频文件。

3. 安装 Python 库
我们需要几个强大的 Python 库。在终端中逐个安装它们：

Bash

# 用于分离人声和背景音乐
pip install -U demucs

# 用于说话人日志和识别，功能非常强大
pip install --pre pyannote.audio
pyannote.audio 可能需要你同意 Hugging Face 上的模型使用协议。请确保你有一个 Hugging Face 账户，并已在本地登录。

第二步：(可选) 使用 Demucs 分离人声
为了提高后续识别的准确率，我们可以先尝试把背景音乐去掉。Demucs 是 Facebook Research 出品的效果非常好的工具。

在终端里直接使用命令即可：

Bash

demucs --two-stems=vocals episode_01_audio.wav
运行完毕后，会在一个名为 separated/htdemucs 的文件夹里找到一个 vocals.wav 文件，这就是分离出来的人声。接下来的步骤我们将基于这个文件进行。

第三步：使用 Pyannote.audio 进行说话人日志
现在，我们要分析 vocals.wav 文件，看看里面有几个人在说话，以及他们各自说话的时间段。

代码解释：

这段代码会加载一个预训练好的“说话人日志”模型。

你需要从 Hugging Face 获取一个访问令牌 (Access Token) 才能使用这些模型。请登录你的Hugging Face账户，在 Settings -> Access Tokens 页面创建一个。

pipeline 对象会处理音频文件，并返回一个 annotation 对象，里面包含了每个说话片段的时间戳和分配的说话人标签（如 SPEAKER_00, SPEAKER_01 等）。

代码实现：
创建一个名为 diarize.py 的文件。

Python

import torch
from pyannote.audio import Pipeline
import os

# --- 配置 ---
# 1. 替换成你自己的 Hugging Face 访问令牌
HF_TOKEN = "YOUR_HUGGING_FACE_TOKEN"
# 2. 指定上一步分离出的人声音频文件
INPUT_AUDIO = "separated/htdemucs/vocals.wav"
# 3. 指定输出的说话人日志文件名
OUTPUT_LOG = "diarization_log.txt"

# 检查文件是否存在
if not os.path.exists(INPUT_AUDIO):
    print(f"错误: 输入文件 '{INPUT_AUDIO}' 未找到。")
    print("请先运行第一步和第二步。")
    exit()

print("正在加载说话人日志模型...")
# 使用你的令牌加载模型
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=HF_TOKEN
)

# 如果有GPU，将模型移至GPU以加速
if torch.cuda.is_available():
    print("检测到GPU，将使用GPU进行处理。")
    pipeline.to(torch.device("cuda"))
else:
    print("未检测到GPU，将使用CPU进行处理（可能较慢）。")

print(f"正在处理音频文件: {INPUT_AUDIO}...")
# 运行说话人日志流程
diarization = pipeline(INPUT_AUDIO)

print("处理完成！正在将结果写入日志文件...")
# 将结果写入文本文件，方便查看
with open(OUTPUT_LOG, "w") as log_file:
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        # turn.start 和 turn.end 是说话的开始和结束时间（秒）
        log_line = f"开始: {turn.start:.2f}s | 结束: {turn.end:.2f}s | 说话人: {speaker}\n"
        print(log_line, end="")
        log_file.write(log_line)

print(f"\n说话人日志已保存到 '{OUTPUT_LOG}'。")
运行这个脚本：
python diarize.py

完成后，你会得到一个 diarization_log.txt 文件，内容类似：

开始: 2.53s | 结束: 5.89s | 说话人: SPEAKER_01
开始: 6.12s | 结束: 8.45s | 说话人: SPEAKER_00
开始: 9.01s | 结束: 12.77s | 说话人: SPEAKER_01
...
第四步：识别哪个说话人是宋江
现在我们有了不同说话人的片段，但还不知道 SPEAKER_00, SPEAKER_01... 哪个才是宋江。

1. 准备宋江的声音样本
你需要手动从电视剧中剪辑一小段 只有宋江在说话，且没有背景音乐 的音频，时长 5-10 秒即可，保存为 lord_sample.wav。这个样本的质量至关重要！

2. 编写脚本进行识别和切分
接下来的脚本会比较复杂。它会：

读取上一步生成的日志文件。

为 lord_sample.wav 创建一个“声纹”向量。

为日志中每一个说话人（SPEAKER_00, SPEAKER_01...）都创建一个声纹向量。

通过计算声纹向量之间的距离（相似度），找出与宋江样本最接近的那个说话人标签。

最后，使用 FFmpeg 和日志文件，将所有属于宋江的片段从原音频中剪切出来。

这一步的代码比较高级，因为它整合了多个库。你需要先安装 pydub 和 speechbrain：
pip install pydub speechbrain

代码实现 identify_and_export.py: (这是一个概念性的高级脚本)

Python

import os
import torch
from speechbrain.pretrained import SpeakerRecognition
from pydub import AudioSegment
import numpy as np

# --- 配置 ---
DIARIZATION_LOG = "diarization_log.txt"
ORIGINAL_VOCALS = "separated/htdemucs/vocals.wav"
LORD_SAMPLE = "lord_sample.wav" # 你手动准备的样本
OUTPUT_DIR = "lord_clips"
FINAL_OUTPUT = "lord_voice_compilation.mp3"

# --- 准备工作 ---
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# 加载声纹识别模型
print("正在加载声纹识别模型...")
verification = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir="pretrained_models/spkrec-ecapa-voxceleb"
)

# --- 函数定义 ---
def get_embedding(audio_file):
    """为音频文件创建声纹嵌入（向量）"""
    signal, fs = torchaudio.load(audio_file)
    return verification.encode_batch(signal)

def cosine_similarity(emb1, emb2):
    """计算两个嵌入的余弦相似度"""
    return torch.nn.functional.cosine_similarity(emb1, emb2).item()

# --- 主逻辑 ---
# 1. 创建宋江的声纹
print(f"正在为样本 '{LORD_SAMPLE}' 创建声纹...")
lord_embedding = get_embedding(LORD_SAMPLE)

# 2. 加载音频和日志，为每个说话人创建平均声纹
print("正在分析日志文件并为每个说话人创建声纹...")
audio = AudioSegment.from_wav(ORIGINAL_VOCALS)
speaker_embeddings = {}
with open(DIARIZATION_LOG, 'r') as f:
    for line in f:
        parts = line.strip().split(" | ")
        start_ms = float(parts[0].split(": ")[1][:-1]) * 1000
        end_ms = float(parts[1].split(": ")[1][:-1]) * 1000
        speaker = parts[2].split(": ")[1]

        # 提取片段并创建声纹
        segment = audio[start_ms:end_ms]
        # pydub/speechbrain 交互需要临时文件
        segment.export("temp_segment.wav", format="wav")
        embedding = get_embedding("temp_segment.wav")

        if speaker not in speaker_embeddings:
            speaker_embeddings[speaker] = []
        speaker_embeddings[speaker].append(embedding)

# 计算每个说话人的平均声纹
avg_embeddings = {
    spk: torch.mean(torch.cat(embs), dim=0, keepdim=True)
    for spk, embs in speaker_embeddings.items()
}

# 3. 比较相似度，找到宋江
print("正在匹配最相似的说话人...")
best_match_speaker = None
highest_similarity = -1

for speaker, emb in avg_embeddings.items():
    similarity = cosine_similarity(lord_embedding, emb)
    print(f"与 {speaker} 的相似度: {similarity:.4f}")
    if similarity > highest_similarity:
        highest_similarity = similarity
        best_match_speaker = speaker

if best_match_speaker:
    print(f"\n识别完成！'{best_match_speaker}' 最有可能是宋江。")
else:
    print("未能找到匹配的说话人。")
    exit()

# 4. 导出所有宋江的片段并合并
print("正在导出并合并所有宋江的音频片段...")
lord_compilation = AudioSegment.empty()
with open(DIARIZATION_LOG, 'r') as f:
    for i, line in enumerate(f):
         if best_match_speaker in line:
            parts = line.strip().split(" | ")
            start_ms = float(parts[0].split(": ")[1][:-1]) * 1000
            end_ms = float(parts[1].split(": ")[1][:-1]) * 1000
            clip = audio[start_ms:end_ms]
            lord_compilation += clip

# 导出最终文件
lord_compilation.export(FINAL_OUTPUT, format="mp3")
print(f"任务完成！所有宋江的声音已合成为 '{FINAL_OUTPUT}'。")
os.remove("temp_segment.wav") # 清理临时文件
注意：上面这段代码非常高级，直接运行前需要确保所有依赖和模型都已正确安装和配置，并且音频格式和采样率都符合 speechbrain 的要求。它主要用于展示完整的逻辑流程。

总结与建议
这是一个专业级的任务，每一步都可能遇到挑战。

结果不会100%完美：即使使用了最先进的模型，当多人同时说话、背景噪音过大时，分离和识别的效果都会下降。

从简单开始：我强烈建议你先从**第一步（提取音频）和第二步（分离人声）**开始，确保能成功运行并得到结果。

逐步推进：然后尝试运行第三步（说话人日志），看看 pyannote.audio 是否能正确区分出不同的人。

关键样本：第四步的成功与否，极度依赖你提供的 lord_sample.wav 的质量。一定要找一段最清晰、最纯净的独白。

这个项目非常有挑战性，但也能让你学到大量关于音频处理和AI应用的知识。如果在任何一步卡住了，随时把你的问题和错误信息告诉我，我会帮你分析解决！